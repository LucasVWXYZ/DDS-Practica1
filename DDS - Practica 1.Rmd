---
title: "DDS - Práctica 1"
output:
  pdf_document: default
  html_document: default
date: "2024-01-09"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Driven Security - Actividad Evaluable 1

## Víctor López García & Lucas Carrillo Mas

### 1. Data Science - Parte teórica

Se listarán a continuación los enunciados de las preguntas requeridas junto con su respuesta.

#### Pregunta 1 - De las siguientes preguntas, clasifica cada una como descriptiva, exploratoria, inferencia, predictiva o causal, y razona brevemente (una frase) el porqué:

##### 1. Dado un registro de vehículos que circulan por una autopista, disponemos de su marca y modelo, país de matriculación, y tipo de vehículo (por número de ruedas). Con tal de ajustar precios de los peajes,

##### - ¿Cuántos vehículos tenemos por tipo?

Descriptiva, porque describe o explica el conjunto de datos.

##### - ¿Cuál es el tipo más frecuente?

Exploratoria, porque pretende explorar relaciones en los datos

##### - ¿De qué países tenemos más vehículos?

Exploratoria, porque relaciona vehículos con paises y su cantidad.

##### 2. Dado un registro de visualizaciones de un servicio de video-on-demand, donde disponemos de los datos del usuario, de la película seleccionada, fecha de visualización y categoría de la película, queremos saber, ¿Hay alguna preferencia en cuanto a género literario según los usuarios y su rango de edad?

Exploratoria porque pretende buscar preferencias en el género literario con respecto a la variable edad.

##### 3. Dado un registro de peticiones a un sitio web, vemos que las peticiones que provienen de una red de telefonía concreta acostumbran a ser incorrectas y provocarnos errores de servicio.

##### - ¿Podemos determinar si en el futuro, los próximos mensajes de esa red seguirán dando problemas?

Predictiva, porque pretende hacer una predicción en base a los datos que tenemos.

##### - ¿Hemos notado el mismo efecto en otras redes de telefonía?

Inferencial, porque busca hacer una abstracción a más redes de telefonía.

##### 4. Dado los registros de usuarios de un servicio de compras por internet, los usuarios pueden agruparse por preferencias de productos comprados. Queremos saber si ¿Es posible que, dado un usuario al azar y según su historial, pueda ser directamente asignado a un o diversos grupos?

Predictiva, porque en base a nuestros datos, queremos hacer una predicción sobre la actuación de un usuario al azar.

#### Pregunta 2 - Considera el siguiente escenario:

##### Sabemos que un usuario de nuestra red empresarial ha estado usando esta para fines no relacionados con el trabajo, como por ejemplo tener un servicio web no autorizado abierto a la red (otros usuarios tienen servicios web activados y autorizados). No queremos tener que rastrear los puertos de cada PC, y sabemos que la actividad puede haber cesado. Pero podemos acceder a los registros de conexiones TCP de cada máquina de cada trabajador (hacia donde abre conexión un PC concreto). Sabemos que nuestros clientes se conectan desde lugares remotos de forma legítima, como parte de nuestro negocio, y que un trabajador puede haber habilitado temporalmente servicios de prueba. Nuestro objetivo es reducir lo posible la lista de posibles culpables, con tal de explicarles que por favor no expongan nuestros sistemas sin permiso de los operadores o la dirección.

#### Explica con detalle cómo se podría proceder al análisis y resolución del problema mediante Data Science, indicando de donde se obtendrían los datos, qué tratamiento deberían recibir, qué preguntas hacerse para resolver el problema, qué datos y gráficos se obtendrían, y cómo se comunicarían estos.

En primer lugar, deberíamos de identificar los datos que nos serían útiles para este análisis. Entre ellos, destacarían los registros de conexiones TCP de las máquinas de cada trabajador por contener información como direcciones IP, puertos, timestamps, etc. de cada conexión.

A continuación, en la parte de tratamiento de datos, debemos de limpiar estos registros identificando posibles valores atípicos, errores o datos faltantes para garantizar la consistencia de los datos y su integridad.

Una vez hemos conseguido datos elegantes, procedemos a analizarlos. Exploraríamos la distribución de conexiones TCP en función del tiempo con el objetivo de identificar patrones o comportamientos inusuales que podrían indicar actividades no autorizadas. En concreto, podríamos buscar patrones de conexión que se desvíen significativamente del comportamiento normal, considerando comportamiento normal el de los registros que acceden a servicios web autorizados. También podríamos simplemtente buscar por conexiones a servicios no autorizados. Antes de proceder a la representación, debemos de comprobar las conexiones que podrían considerarse excepciones y requerir una explicación. Un análisis más avanzado podría emplear modelos de detección de anomalías con machine learning para identificar conexiones que se aparten significativamente del comportamiento normal de forma automatizada.

Finalmente, para comunicar las conclusiones de una forma amigable para la alta dirección de la empresa, crearíamos presentaciones o dashboards en los que utilizaríamos gráficos temporales que muestren la actividad de conexiones TCP a lo largo del tiempo, destancando estos eventos anómalos o inusuales en las mismas representaciones.

### 2. Introducción a R - Parte práctica

```{r carga paquetes}
library(tidyverse)
library(readr)
library(stringr)
library(tidyr)
```

```{r carga datos}
Logs_http <- read_delim("epa-http.csv", delim = " ", show_col_types = FALSE)
```

```{r limpieza datos}
colnames(Logs_http) <- c("source", "timestamp", "petition", "status", "bytes")
Logs_http <- separate(Logs_http, petition, into = c("type", "url", "protocol"), sep = " ")
Logs_http <- Logs_http %>%
  mutate(
    # source se mantiene como caracteres
    #timestamp = strptime(timestamp, format = "[%d:%H:%M:%s]"),  # Cambiar la columna timestamp a fecha
    type = as.factor(type),  # Cambiar la columna type a factor
    # url se mantiene como caracteres
    # protocol se mantiene como caracteres
    status = as.factor(status),  # Cambiar la columna status a factor
    bytes = as.integer(bytes)  # Cambiar la columna port a entero
  )

head(Logs_http)
```

```{r visualización dataframe}
View(Logs_http)
```

Empezamos con los ejercicios de la práctica, que listaremos a continuación junto con su resolución en formato código.

#### Pregunta 1

1.  Cuales son las dimensiones del dataset cargado (número de filas y columnas)

```{r análisis preliminar}
dimensiones_dataframe <- dim(Logs_http)
print(dimensiones_dataframe)
```

Lo que nos dice que el dataframe tiene 47747 filas y 7 columnas (después de separar la columna "petition" en 3).

2.  Valor medio de la columna Bytes

```{r análisis descriptivo}
media_bytes <- mean(Logs_http$bytes, na.rm=TRUE)
print(media_bytes)
```

#### Pregunta 2

De las diferentes IPs de origen accediendo al servidor, ¿cuantas pertenecen a una IP claramente educativa (que contenga ".edu")?

```{r IP edu}
numIP_edu <- sum(grepl(".edu", Logs_http$source))
numIPunicas_edu <- sum(grepl(".edu", unique(Logs_http$source)))
print(numIP_edu)
print(numIPunicas_edu)
```

Vemos que hay 6539 logs en los que IPs educativas acceden al servidor, pero de ellas, solamente 376 IPs son únicas.

#### Pregunta 3

De todas las peticiones recibidas por el servidor cuál es la hora en la que hay mayor volumen de peticiones HTTP de tipo "GET"?

```{r hora peticiones GET}
separacion_hora <- separate(Logs_http, timestamp, into = c("day", "hour", "minute", "second"), sep = ":")
hora <- separacion_hora$hour

hora_fact <- factor(separacion_hora$hour)

frecuencias <- table(hora_fact)
hora_maxpeticiones <- names(frecuencias)[which.max(frecuencias)] #Buscamos la hora con más frecuencia en la tabla y sacamos el "nombre" (será la hora). 

print(frecuencias)
print(hora_maxpeticiones)

```

En primer lugar, vamos a suponer que se nos pide qué hora a lo largo de todo un día completo (24 horas) es la que tiene más peticiones HTTP de tipo "GET". Vemos que tenemos logs desde las 23:53:36 del día 29 hasta las 23:53:07 del día 30, por lo que realmente ya es una distribución representativa de 24 horas y no hace falta hacer ninguna corrección más, ya que no hay solapes ni otras problemáticas.

Entonces, es suficiente con coger el valor "hora" de la columna "timestamp", que sería el segundo conjunto de números en el formato dado [dd:hh:mm:ss]. Para ello, una posible forma (aunque bastante rudimentaria) es hacer una separación por ":" y coger la segunda columna.

A continuación, tranformamos la columna "hour" a factores del 00 al 23 y hacemos una tabla de frecuencias para contar cuántas entradas hay para cada hora.

Vemos que la hora a la que se realizan más peticiones GET es las 14h, es decir las 2 pm.

#### Pregunta 4

De las peticiones hechas por instituciones educativas (.edu), ¿Cuantos bytes en total se han transmitido, en peticiones de descarga de ficheros de texto ".txt"?

```{r bytes edu txt}
IPedu <- grepl(".edu", Logs_http$source)
IPtxt <- grepl(".txt", Logs_http$url)
IP_tot <- IPedu & IPtxt
sum_bytes <- sum(Logs_http[IP_tot,"bytes"], na.rm = TRUE)
print(sum_bytes)

#numIP_edu <- sum(grepl(".edu", Logs_http$source))
#numIPunicas_edu <- sum(grepl(".edu", unique(Logs_http$source)))
#print(numIP_edu)
#print(numIPunicas_edu)
```

Vemos que el tamaño total de bytes para peticiones de IPs educativas de descarga de ficheros de texto ".txt" es 3017871 bytes. Notar que como en la columna "bytes" había valores no disponibles (NA), hemos tenido que eliminarlos para esta suma.

#### Pregunta 5

Si separamos la petición en 3 partes (Tipo, URL, Protocolo), usando str_split y el separador " " (espacio), ¿cuantas peticiones buscan directamente la URL = "/"?

```{r busqueda a raiz (/)}
num_raiz <- sum(Logs_http$url == "/")

print(num_raiz)
```

El total de peticiones a la raiz del dominio es 2382.

#### Pregunta 6

Aprovechando que hemos separado la petición en 3 partes (Tipo, URL, Protocolo) ¿Cuantas peticiones NO tienen como protocolo "HTTP/0.2"?

```{r total no protocolos HTTP/0.2}
num_protocols <- sum(Logs_http$protocol != "HTTP/0.2")
unique_protocols <- unique(Logs_http$protocol)

print(num_protocols)
```

Vemos que hay 47746 peticiones con protocolo diferente a HTTP/0.2 (es decir, con protocolo HTTP/1.0, ya que vimos que solo había estos dos tipos de protocolos en nuestros logs), por lo que tenemos una sola petición con protocolo HTTP/0.2.
